---
title: "SkaskoStephen_HW7"
author: "Stephen Skasko"
date: "4/3/2023"
output: html_document
---

#2

Often d = 1 works well, in which case each
tree is a stump, consisting of a single split. In this case, the boosted stump ensemble is fitting an additive model, since each 
term involves only a single variable. 

#4

                   x1 < 1
                      |
                    -----
                   |     |
                x2 < 1   5
                 -----
                |     | 
             x1 < 0   15
             -----
            |     | 
            3  x2 < 0 
                -----
               |     | 
              10     0
              
              
   |------------------|
   |      2.49        |
 2 |------------------| 
   |   -1.06 | 0.21   | x2
 1 |------------------|
   |  1.80    | 0.63  |
   |------------------|
          x1
#5 

Vote approach:  Red > 0.5, green < 0.49

Red = 6 
Green = 4
Final classification is Red


Average approach: 
0.1 + 0.15 + 0.2 + 0.55 + 0.6 + 0.6 + 0.65 + 0.7 + 0.75 / 10 = 0.45 --> red, 1-0.45 = 0.55 --> green. final classification is green


#3

#8
a)
```{r}

library (ISLR2)
attach (Carseats)
 

Carseats$Sales <- as.numeric(Carseats$Sales)

set.seed(1)
train <- sample(1:nrow(Carseats), size = round(0.7 * nrow(Carseats)), replace = FALSE)
trainSet <- Carseats[train, ]
testSet <- Carseats[-train, ]
```

b) 
Results: seems like showing sales towards left sides of the tree gives off more identification in the different variable sets.

MSE: 4.01
```{r}
library(rpart)
library(rpart.plot)

R.tree <- rpart(Sales ~ ., data = trainSet )
rpart.plot(R.tree)

#MSE Test
pred <- predict(R.tree, newdata = testSet)
mse <- mean((testSet$Sales - pred)^2)
mse



```
c)
MSE: 5.11
```{r}
library(tree)

set.seed(1)
cv <- cv.tree(tree(Sales ~ ., data = trainSet))
plot(cv$size, cv$dev, type = "b")
prune <- prune.tree(tree(Sales ~ ., data = trainSet), best = 6)
plot(prune)
text(prune)

#MSE Test
predN <- predict(prune, newdata = testSet)
mseN <- mean((testSet$Sales - predN)^2)
mseN

```

d)
MSE: 2.57
Most Important: Price and ShelveLoc 
```{r}
library(randomForest)
set.seed(1)
bag <- randomForest(Sales ~ ., data = trainSet, mtry = 10, importance = TRUE)
pred <- predict(bag, newdata = testSet)
mse <- mean((testSet$Sales - pred)^2)
mse
importance(bag)

```

e)
MSE: 2.91
Most Important: CompPrice, ShevleLoc, Price, Advertising, age
m: seems to show difference in values and compares them. Effect could be correlation of high vs low 
```{r}

set.seed(1)
rf <- randomForest(Sales ~ ., data = trainSet, mtry = 3, importance = TRUE)
pred <- predict(rf, newdata = testSet)

# MSE
mse <- mean((testSet$Sales - pred)^2)
mse
importance(rf)

```

f)
MSE: 13.9
```{r}
library (BART)
set.seed (1)

bart <- gbart(trainSet[, -1], trainSet$Sales, x.test = testSet[, -1])

# Calculate MSE
yhat.bart <- bart$yhat.test.mean
mean((trainSet$Sales - yhat.bart)^2)

```

#10
a)
```{r}
library(ISLR)
data("Hitters")
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)

```
b)
```{r}
train <- Hitters[1:200, ]
test <- Hitters[-(1:200), ]

```

c)
```{r}
library(gbm)

shrinkage.values <- c(-0.5, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 1)
train.mse <- c()
for (i in shrinkage.values) {
  fit <- gbm(Salary ~ ., data = train, n.trees = 1000, interaction.depth = 4, shrinkage = i, verbose = FALSE)
  train.pred <- predict(fit, newdata = train, n.trees = 1000)
  train.mse <- c(train.mse, mean((train$Salary - train.pred)^2))
}
plot(shrinkage.values, train.mse, type = "b", xlab = "Shrinkage", ylab = "Training MSE")
  
```

d)
```{r}
test.mse <- c()
for (i in shrinkage.values) {
  fit <- gbm(Salary ~ ., data = train, n.trees = 1000, interaction.depth = 4, shrinkage = i, verbose = FALSE)
  test.pred <- predict(fit, newdata = test, n.trees=1000)
  test.mse <- c(test.mse, mean((test$Salary - test.pred)^2))
}
plot(shrinkage.values, test.mse, type="b", xlab="Shrinkage", ylab="Test MSE")

```

e)
```{r}
# Linear regression
lm.fit <- lm(Salary ~ ., data = train)
lm.test.pred <- predict(lm.fit, newdata = test)
lm.test.mse <- mean((test$Salary - lm.test.pred)^2)
lm.test.mse
# Ridge regression
library(glmnet)

x_train <- model.matrix(Salary ~ ., data = train)
y_model <- model.matrix(Salary ~ ., data = test)
y_train <- train$Salary

ridge_fit <- glmnet(x_train, y_train, alpha = 0)
ridge_test_pred <- predict(ridge_fit, newx = y_model, s = 0.01 )
ridge_test_mse <- mean((test$Salary - ridge_test_pred)^2)
ridge_test_mse
# Boosting
boost_fit <- gbm(Salary ~ ., data = train, n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = FALSE)
bo.test.pred <- predict(boost_fit, newdata = test)
bo.test.mse <- mean((test$Salary - bo.test.pred)^2)
bo.test.mse

```

f)
Looks like the most important is CAtBat, CWalks and PutOuts as they have the highest values in the plot.
```{r}
summary(boost_fit)
```

g)
The MSE is 0.218
```{r}
bag_fit <- randomForest(Salary ~ ., data = train, n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = FALSE)
ba.test.pred <- predict(bag_fit, newdata = test)
ba.test.mse <- mean((test$Salary - ba.test.pred)^2)
ba.test.mse

```

#4
Its preferably over a classification tree because of interchangeability, flexibility, and for better predictions. 

Interchangeability: Regression trees provide easier to explain compared to classification trees when it comes to many predictors and can be visualized with a split based system so its easier to understand the importance of certain predictors. 
Flexibility: Regression trees are both quantitative and qualitative predictors and a classification tree can only handle qualitatives predictors. Being able to have quantitative predictors gives us more information to work with in the tree and improve the model's accuracy.
Better predictions: Regression trees can be used for binary classification by setting a theshold on the predicted values, leading to better predictions than a classification tree since the tree captures more relationships for between predictors and responses. 

#5
a)
```{r}
df.train <- read.csv("HW7train.csv", header = TRUE)
set.seed(1)
train <- sample(nrow(df.train), 900)
trainSet <- df.train[train, ]
testSet <- df.train[-train, ]

```

b)
Important predictors seem to be X1, X2, and X3
```{r}
library(randomForest)

rf <- randomForest(y ~ ., data = trainSet, importance = TRUE)
par(mfrow=c(3,1))
plot(rf$importance[,1],type="b",axes=F,ann=F,ylim=c(0,max(rf$importance[,1])+1))
axis(1,at=1:10,lab=names(df.train)[-1])
axis(2,at=seq(0,max(rf$importance)+1,0.25),las=1)
box()


```

c) 
Important: X1, X2, X3 seem to still be important predictors.
```{r}
mse.perm <- rep(NA, 10)
for (i in 1:10) {
  perm.testSet <- testSet
  perm.testSet[, i] <- sample(testSet[, i])
  perm.pred <- predict(rf, newdata = perm.testSet)
  mse.perm[i] <- mean((testSet$y - perm.pred)^2)
}

plot(mse.perm,type="b",axes=F,ann=F,ylim=c(0,max(mse.perm)+1))
axis(1,at=1:10,lab=names(df.train)[-1])
axis(2,at=seq(0,max(mse.perm)+1,0.25),las=1)
box()
```

d)
Looks more like c) graph and I would trust c) more. 
```{r}
library(randomForest)
mse.loo <- rep(NA, 10)
for (i in 1:10) {

  loo.rf <- randomForest(y ~ ., data = trainSet, importance = TRUE)
  loo.pred <- predict(loo.rf, newdata = testSet)
  mse.loo[i] <- mean((testSet$y - loo.pred)^2)
}
plot(mse.loo,type="b",axes=F,ann=F,ylim=c(0,max(mse.loo)+1))
axis(1,at=1:10,lab=names(df.train)[-1])
axis(2,at=seq(0,max(mse.loo)+1,0.25),las=1)
box()

```
e)
The difference might be caused by the high correlations between the predictors or have multicollinearity between the predictors.

#6
a)
```{r}
set.seed(1)
n <- 100
p <- 10
sigma <- 1
X <- matrix(rnorm(n*p), n, p)
epsilon <- rnorm(n, 0, sigma)
Y <- rowSums(X) + epsilon
trainSet <- data.frame(Y, X)

```

b)
```{r}
set.seed(1)
n.test <- 10000
x.test <- matrix(rnorm(n.test*p), n.test, p)
ep.test <- rnorm(n.test, 0, sigma)
y.test <- rowSums(x.test) + ep.test
test.set <- data.frame(y.test, x.test)


```

c)
```{r}
set.seed(1)

#bagging
bagging <- randomForest(Y ~ ., data = trainSet, mtry = p, importance = TRUE)
pred_bag <- predict(bagging, newdata = testSet)
Err_bag <- mean((y.test - pred_bag)^2)

#random forest
rf <- randomForest(Y ~ ., data = trainSet, mtry = 3, importance = TRUE)
pred.rf <- predict(rf, newdata = testSet)
Err_rf <- mean((y.test - pred.rf)^2)

```

d)
```{r}
set.seed(1)

n.rec <- 50
Err_bagg <- numeric(n.rec)
Err_rf <- numeric(n.rec)

for (i in 1:n.rec) {
  X <- matrix(rnorm(n * p), nrow = n)
  Y <- apply(X, 1, sum) + rnorm(n, mean = 0, sd = sigma)
  data <- data.frame(Y, X)
  
  X.test <- matrix(rnorm(n.rec * p), nrow = n.rec)
  Y.test <- apply(X.test, 1, sum) + rnorm(n.rec, mean = 0, sd = sigma)
  data.test <- data.frame(Y.test, X.test)
  
bagging <- randomForest(Y ~ ., data = data, mtry = p, importance = TRUE)
  pred_bag <- predict(bagging, newdata = data.test)
  Err_bagg[i] <- mean((Y.test - pred_bag)^2)

rf <- randomForest(Y ~ ., data = data, mtry = 3, importance = TRUE)
  pred_rf <- predict(rf, newdata = data.test)
  Err_rf[i] <- mean((Y.test - pred_rf)^2)
}
mean_err_bagg <- mean(Err_bagg)
mean_err_rf <- mean(Err_rf)

```

e)
We see a plot that jumps a lot and when noise level is low then we see the difference closer downwards, so they are similar but bagging performs a little since it works better in high noise environments. I believe bagging is the  drop below 0 and random forest is the opposite showing the variance differences over samples.

Bagging performs well with high noises as well.
```{r}
plot(Err_bagg - Err_rf, type = "l")
abline(h = 8, col = "red")
```
